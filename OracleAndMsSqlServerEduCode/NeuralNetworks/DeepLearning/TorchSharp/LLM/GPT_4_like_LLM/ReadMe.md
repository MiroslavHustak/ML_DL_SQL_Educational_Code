Access to the GPT-4-like code is restricted.

Nemá někdo (nejlépe z ČR, SR, PL, AT, HU, to abychom neměli k sobě daleko) chuť se přidat k mému side-projektu "mini LLMka" v tomto .NET jazyce (https://github.com/MiroslavHustak/FAQ) a TorchSharp pro lokální fine-tuning dat, která nemohou opustit "bezpečnostní prostor" uživatele? Nemohu slíbit, že to bude fungovat dle mých představ, ale pokud ano, můžeme uvažovat o společné monetizaci projektu. Bude to založeno na tomto ["GPT-2-like" kódu](https://github.com/MiroslavHustak/ML_DL_SQL_Educational_Code/tree/master/OracleAndMsSqlServerEduCode/NeuralNetworks/DeepLearning/TorchSharp/LLM/GPT-2-like_LLM) (architekturou, ne výkonem) s přidanými "GPT-3 a GPT-4-like" prvky, už jsem do kódu v private repository přidal rotary positional embeddings, root mean square layer normalization, LoRA, batching a přípravu pro learning rate scheduler (vše bez optimalizace parametrů). Zbývá ještě minimálně flash attention a fuse ops (zřejmě bude nutné to udělat v C++, rád bych sice Rust, který se učím, ale asi to v něm nepůjde), implementace real-world tokenizátoru (mám jen simulaci), implementace third-party weights, kód pro zpracování dat pro fine-tuning. Konkurence samozřejmě existuje (např. Hugging Face Transformers + PEFT + vLLM nebo TensorRT-LLM, a hlavně pak Ollama + lokální fine-tuning či dnes už možná budou uvolněné lokální modely od OpenAI a dalších firem). U konkurence jsou omezené možnosti ovlivnit model (možnost zadat jen ty parametry, které jiní dovolí) a je tam závislost na cizím řešení, s vlastním mini LLM máme vše pod kontrolou, včetně dalšího rozvoje. Bude to chtít společným úsilím sehnat přístup k PC s CUDA-compatible GPU. Byla by dobrá schopnost naučit se něco málo z matematiky (stačí jen to, co se učí v prvních čtyřech semestrech na technické VŠ). Více info pak samozřejmě pošlu. Prosím ozvěte se přes DM nebo email uvedený v kontaktech jen v případě vážného zájmu. Díky :-).
