Access to the GPT-4-like code is restricted.

Nemá někdo (nejlépe z ČR, SR, PL, AT, HU, to abychom neměli k sobě daleko) chuť se přidat k mému side-projektu "mini LLMka" pro .NET v tomto .NET jazyce (https://github.com/MiroslavHustak/FAQ) a TorchSharp pro lokální fine-tuning dat (pro .NET-only prostředí), která nemohou opustit "bezpečnostní prostor" uživatele? Nemohu slíbit, že to bude fungovat dle mých představ, ale pokud ano, můžeme uvažovat o společné monetizaci projektu. Bude to založeno na tomto ["GPT-2-like" kódu](https://github.com/MiroslavHustak/ML_DL_SQL_Educational_Code/tree/master/OracleAndMsSqlServerEduCode/NeuralNetworks/DeepLearning/TorchSharp/LLM/GPT-2-like_LLM) (architekturou, ne výkonem) s přidanými prvky používaných v moderních modelech, už jsem do kódu v private repository přidal rotary positional embeddings, root mean square layer normalization, LoRA, batching a přípravu pro learning rate scheduler (vše bez optimalizace parametrů). Zbývá ještě minimálně flash attention a fuse ops (zřejmě bude nutné to udělat v C++, rád bych sice Rust, který se učím, ale nevím, jestli to v něm půjde), implementace real-world tokenizátoru (zatím mám jen jednoduchou simulaci), implementace dostupných open weights, kód pro zpracování dat pro fine-tuning. Konkurence samozřejmě existuje (např. Hugging Face Transformers + PEFT + vLLM nebo TensorRT-LLM, a hlavně pak Ollama + lokální fine-tuning či dnes už možná budou uvolněné lokální modely od OpenAI a dalších firem). U konkurence se dá očekávat, že bude příliš komplexní pro malé projekty – vlastní mini-framework umožní plnou kontrolu nad architekturou a experimenty. Bude to chtít společným úsilím sehnat přístup k PC s CUDA-compatible GPU. Prosím ozvěte se přes DM nebo email uvedený v kontaktech jen v případě vážného zájmu. Díky :-).
